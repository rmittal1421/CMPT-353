1. In your reddit_relative.py, what intermediate results did you .cache()? Briefly describe what would have happened if you hadn't used .cache()
anywhere. (No need to time it, unless you really want to.)
A. The values I cached are:
1. The dataframe called comments when I am getting spark to read it from json file (line 36).
2. After calculating the relative score for each subreddit and adding a corresponding column to comments, I am again caching in the updated 
dataframe 'comments' (line 43).

Explanation: I am caching at the times mentioned above because of the spark's lazy evaluation and the way the physical plans are laid out for the 
variables. To compute any particular value, spark does it in the lazy fashion and does not evaluate until it has to but just keeps adding the 
operation to it's physical plan. And once the value is calculated, it does not see that this value might be needed in the future and hence, tosses
the result out which could have been used again in the future. So if that value is needed again, it relies on same physical plan and repeats every
step again to get that same value which was calculated previously. Therefore, if I hadn't used .cache() at the above mentioned places, my program 
would have done duplicate calculations multiple times resulting in increased runtime and wasted resources.

2. How did marking DataFrames for broadcast affect the running time of the “best author” program above?
A. 