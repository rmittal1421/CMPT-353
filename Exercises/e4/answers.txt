1. Based on your results for the last question, do you think daily temperatures are a good way to predict population density? Briefly explain why or why not.
A. No, I do not think that there is any relationship between daily temperatures and population density of a city and hence the population density of a particular
city cannot be predicted by looking at daily temperatures. As we can see in the plot generated, it does not make sense to plot a best fit line on this graph since
the slope of the fit line is not going to explain any direct relations. Most of the temperatures lies between 10-30 degrees even if the city polulation density varies
from 0-2000 people per squared kilometer. This helps us conclude that there is no real relationship between average temperatures and population density of an area/city.

2. The larger data file (stations.json.gz) was kept compressed on disk throughout the analysis. Decompressing every time we run the program seems inefficient. 
Why might this be faster than working with an uncompressed .json data?
A. It looks like the task of getting the compressed version of the file from storage and then decompressing it in main memory on every run time looks inefficient in 
whereas keep it in the decompressed version in storage and then load it directly in the main memory looks faster. But in reality, if we keep our data in decompressed 
manner in our storage, we need to perform a huge number of I/O operations while loading that data in the main memory to work on it. On the other hand, if we just keep
the data in compressed version and load the compressed file in the main memory, this process is going to be much faster as it requires very few I/O operations (since the
file size is really small now) and hence it is faster. The overhead of decompressing it in the main memory is negligible in front of I/O operations we performed in the other
case. Hence, it is not slower. Instead, it is much faster to keep it compressed and decompress it on every run time.
